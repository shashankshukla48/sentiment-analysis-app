{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "data frame and operation",
   "id": "79e706b748150d56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "file_path = r'C:\\Users\\Shashank Shukla\\sentiment_analysis\\data\\IMDB Dataset.csv\\IMDB Dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "print(\"data frame loaded\")\n",
    "#lowercase data\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "#remove html tags\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?.>')\n",
    "    return pattern.sub('', text)\n",
    "#remove puntuations\n",
    "def remove_puntuation(text):\n",
    "    return re.sub(r'[^a-z0-9\\s]', ' ', text).strip()# This function uses a regular expression to find and remove any character that is not a lowercase letter (a-z), a digit (0-9), or a whitespace character.\n",
    "#create toen and remove stopwords\n",
    "english_stopword_list = stopwords.words('english')\n",
    "stop_words_set = set(english_stopword_list)\n",
    "def tokenization_and_stopwords_remove(text):\n",
    "    token = text.split()\n",
    "    cleaned_token = [token for token in token if token not in stop_words_set]\n",
    "    return cleaned_token\n",
    "#lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(tokens) for tokens in tokens]\n",
    "    return lemmatize_tokens\n",
    "#rejoin string\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "df['cleaned_review'] = df['review'].apply(to_lowercase).apply(remove_html_tags).apply(tokenization_and_stopwords_remove).apply(lemmatize_tokens).apply(join_tokens)\n",
    "print(df[['review','cleaned_review','sentiment']].head())"
   ],
   "id": "38fd647b0400870d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define Features (X) and Target (y) for Machine Learning",
   "id": "4424a981f03a7fa4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-09T15:45:11.748014Z",
     "start_time": "2025-11-09T15:45:11.733273Z"
    }
   },
   "source": [
    "#x will be the cleaned data and y will be the sentiment to train data\n",
    "x = df['cleaned_review']\n",
    "y = df['sentiment']\n",
    "print('------Features(x)--------')\n",
    "print(x.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"--- Target (y) ---\")\n",
    "print(y.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Features(x)--------\n",
      "0    one reviewer mentioned watching 1 oz episode h...\n",
      "1    wonderful little production. filming technique...\n",
      "2    thought wonderful way spend time hot summer we...\n",
      "3    basically there's family little boy (jake) thi...\n",
      "4    petter mattei's \"love time money\" visually stu...\n",
      "Name: cleaned_review, dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Target (y) ---\n",
      "0    positive\n",
      "1    positive\n",
      "2    positive\n",
      "3    negative\n",
      "4    positive\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:04:17.033480Z",
     "start_time": "2025-11-09T16:04:17.025517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Import the train_test_split Function\n",
    "#split data into training and testing set\n",
    "#Scikit-learn's Dedicated Tool for the Job\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "bdffe33319f125d7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:09:32.789057Z",
     "start_time": "2025-11-09T16:09:32.720946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42,stratify=y)\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ],
   "id": "787c258b9d0a7ae1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (40000,)\n",
      "Shape of x_test: (10000,)\n",
      "Shape of y_train: (40000,)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import TfidfVectorizer for Text Feature Extraction",
   "id": "6e0d951e171338d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:15:06.491768Z",
     "start_time": "2025-11-09T16:15:06.485319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#convert cleaned review data text into meaningful numerical format using NLP(TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "id": "781a8f4817e9fa93",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:24:03.762258Z",
     "start_time": "2025-11-09T16:24:03.733763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#instantiate and configure TfidfVectorizer\n",
    "#creating instance of tfidfvecorizer\n",
    "#create a max_feature that states the vectorizer an limit of 10000 to built a vocabulary and discard everything after that\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "print('Tfidfv=ectorizer initialized successfully')\n",
    "print(tfidf_vectorizer)\n"
   ],
   "id": "54011bb17e020c74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidfv=ectorizer initialized successfully\n",
      "TfidfVectorizer(max_features=10000)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vectorize Training Data using fit_transform",
   "id": "a5294ce60dd8000b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:39:02.111977Z",
     "start_time": "2025-11-09T16:38:57.122871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#train data to vectorize\n",
    "#fit() & transform()\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "print(\"Successfullf fitter the vectorizer and transforemd data\")\n",
    "print(x_train_tfidf)"
   ],
   "id": "ad0f27b524eefada",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfullf fitter the vectorizer and transforemd data\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 3662290 stored elements and shape (40000, 10000)>\n",
      "  Coords\tValues\n",
      "  (0, 1503)\t0.10331129144654669\n",
      "  (0, 5331)\t0.056400916265422915\n",
      "  (0, 3836)\t0.10945163478163895\n",
      "  (0, 9142)\t0.08536266518463241\n",
      "  (0, 207)\t0.11424971247627326\n",
      "  (0, 802)\t0.060778723134354624\n",
      "  (0, 75)\t0.12868698438028828\n",
      "  (0, 7471)\t0.16205435997159454\n",
      "  (0, 8977)\t0.22997787927348404\n",
      "  (0, 7860)\t0.0886572032731704\n",
      "  (0, 9295)\t0.05567433493465328\n",
      "  (0, 6257)\t0.06373241324004299\n",
      "  (0, 8108)\t0.18758717985335624\n",
      "  (0, 7791)\t0.20687842286778038\n",
      "  (0, 3481)\t0.20641901681098912\n",
      "  (0, 5923)\t0.07348527480753918\n",
      "  (0, 6400)\t0.12769709504928503\n",
      "  (0, 3758)\t0.07997384781978854\n",
      "  (0, 9856)\t0.08437959710107112\n",
      "  (0, 9680)\t0.11530098300726328\n",
      "  (0, 8069)\t0.2157519079235348\n",
      "  (0, 1302)\t0.09616840379148249\n",
      "  (0, 8052)\t0.07942957219517927\n",
      "  (0, 8416)\t0.12599428895176093\n",
      "  (0, 9093)\t0.07999081841922956\n",
      "  :\t:\n",
      "  (39999, 5402)\t0.11084287670890154\n",
      "  (39999, 4371)\t0.10493255424656735\n",
      "  (39999, 7417)\t0.1380664805737718\n",
      "  (39999, 1561)\t0.11392874305398838\n",
      "  (39999, 9700)\t0.29347678652460013\n",
      "  (39999, 9872)\t0.13737817544495748\n",
      "  (39999, 7785)\t0.1092499040425904\n",
      "  (39999, 2365)\t0.08925950970047836\n",
      "  (39999, 8628)\t0.15268753342399438\n",
      "  (39999, 8492)\t0.12884828661666428\n",
      "  (39999, 5337)\t0.14604031519680327\n",
      "  (39999, 4409)\t0.17510956620955825\n",
      "  (39999, 3728)\t0.2110021339406272\n",
      "  (39999, 3651)\t0.14517442389509572\n",
      "  (39999, 1684)\t0.1559079456833541\n",
      "  (39999, 4572)\t0.14225461413992846\n",
      "  (39999, 6888)\t0.14865111284108207\n",
      "  (39999, 7934)\t0.15947153451581025\n",
      "  (39999, 1005)\t0.18219620403698922\n",
      "  (39999, 1787)\t0.18687335466955696\n",
      "  (39999, 1990)\t0.20351400921536242\n",
      "  (39999, 2675)\t0.18348103589395323\n",
      "  (39999, 9613)\t0.2110021339406272\n",
      "  (39999, 1679)\t0.17653564723332316\n",
      "  (39999, 7247)\t0.18954444584447822\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:42:20.919670Z",
     "start_time": "2025-11-09T16:42:19.220112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#apply same methon on x_test but we will not fit this time only transform\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "print(\"successfully transformed the test data\")\n",
    "print(x_test_tfidf)"
   ],
   "id": "b6a3fb5b8b6afdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully transformed the test data\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 909806 stored elements and shape (10000, 10000)>\n",
      "  Coords\tValues\n",
      "  (0, 494)\t0.19870677909589626\n",
      "  (0, 996)\t0.07842112392696204\n",
      "  (0, 1359)\t0.11527878074236848\n",
      "  (0, 1565)\t0.14499541671570845\n",
      "  (0, 1744)\t0.14206005575928454\n",
      "  (0, 1834)\t0.14891422792077957\n",
      "  (0, 1877)\t0.12045191180582915\n",
      "  (0, 2376)\t0.12535266288349975\n",
      "  (0, 3130)\t0.11581347513453809\n",
      "  (0, 3142)\t0.11756932740916505\n",
      "  (0, 3209)\t0.08829223196143662\n",
      "  (0, 3213)\t0.10194722340507349\n",
      "  (0, 3255)\t0.14843258960992747\n",
      "  (0, 3454)\t0.09136873130600051\n",
      "  (0, 3547)\t0.06945487450423264\n",
      "  (0, 4436)\t0.14935908776533224\n",
      "  (0, 4836)\t0.11379952590763995\n",
      "  (0, 5185)\t0.10772877690891546\n",
      "  (0, 5267)\t0.15162537579045995\n",
      "  (0, 5284)\t0.05121001266832724\n",
      "  (0, 5374)\t0.09195214729686704\n",
      "  (0, 5413)\t0.11584189017152241\n",
      "  (0, 5474)\t0.06989903052810513\n",
      "  (0, 5503)\t0.06102778495170206\n",
      "  (0, 5565)\t0.17187025005662426\n",
      "  :\t:\n",
      "  (9999, 2784)\t0.15502778921148122\n",
      "  (9999, 3159)\t0.43864107836486455\n",
      "  (9999, 3771)\t0.0907282775909198\n",
      "  (9999, 4144)\t0.17043872662227766\n",
      "  (9999, 5230)\t0.2211469691538413\n",
      "  (9999, 5237)\t0.10974951805969413\n",
      "  (9999, 5289)\t0.18983424465341903\n",
      "  (9999, 5302)\t0.22684039806817222\n",
      "  (9999, 5391)\t0.3256243359843859\n",
      "  (9999, 5403)\t0.08538563605001691\n",
      "  (9999, 5618)\t0.1777494560546319\n",
      "  (9999, 5729)\t0.18246892437151144\n",
      "  (9999, 5813)\t0.1956065877010889\n",
      "  (9999, 5930)\t0.06723661830514387\n",
      "  (9999, 6828)\t0.20434194837323516\n",
      "  (9999, 6905)\t0.1672871009823182\n",
      "  (9999, 7369)\t0.2251762169537738\n",
      "  (9999, 7583)\t0.09446783402050632\n",
      "  (9999, 7926)\t0.10601451770921276\n",
      "  (9999, 8732)\t0.19100448551178364\n",
      "  (9999, 9251)\t0.1438030094205059\n",
      "  (9999, 9324)\t0.16993153276117579\n",
      "  (9999, 9712)\t0.07104667585146447\n",
      "  (9999, 9852)\t0.16845998564717704\n",
      "  (9999, 9912)\t0.06311401159021528\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:44:02.244302Z",
     "start_time": "2025-11-09T16:44:02.237055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, let's check the shape of our training data matrix.\n",
    "print(\"Shape of the TF-IDF training data matrix (X_train_tfidf):\", x_train_tfidf.shape)\n",
    "\n",
    "# Next, we check the shape of our testing data matrix.\n",
    "print(\"Shape of the TF-IDF testing data matrix (X_test_tfidf):\", x_test_tfidf.shape)"
   ],
   "id": "a06919f2500aec10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF training data matrix (X_train_tfidf): (40000, 10000)\n",
      "Shape of the TF-IDF testing data matrix (X_test_tfidf): (10000, 10000)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8cb7e9186c1de4ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
